{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27eaa9cb-c9f1-47f5-9c0c-8b75fc523164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Jobs\n",
    "\n",
    "## Jobs\n",
    "\n",
    "- A Lakeflow Job represents an end-to-end workflow, often comprising multiple steps such as data ingestion, transformation, and loading.\n",
    "- Jobs are defined as a DAG (Directed Acyclic Graph) of tasks.\n",
    "- Each job may include parameters, schedules, and triggers.\n",
    "- Jobs can be reused and version-controlled.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- A Task is a single unit of work in a job.\n",
    "- Tasks can run notebooks, SQL queries, Python scripts, DLT pipelines, or JAR/Python wheel packages.\n",
    "- Tasks may have dependencies on other tasks (divergence and convergence supported).\n",
    "- Tasks can run in parallel or sequentially depending on dependencies.\n",
    "- Configuration includes:\n",
    "  - Task-level parameters\n",
    "  - Retry policy\n",
    "  - Timeout settings\n",
    "  - Cluster or compute type\n",
    "\n",
    "## Compute Types\n",
    "\n",
    "### Interactive Clusters\n",
    "\n",
    "- Manually created and shared by users.\n",
    "- Suitable for ad hoc analysis and testing.\n",
    "- Not recommended for scheduled production jobs.\n",
    "\n",
    "### Job Clusters\n",
    "\n",
    "- Dedicated ephemeral clusters created for each job/task.\n",
    "- Automatically terminated after job completes.\n",
    "- Good for isolation and reproducibility.\n",
    "\n",
    "### Serverless\n",
    "\n",
    "- Managed compute environment.\n",
    "- No cluster configuration needed.\n",
    "- Scales automatically and optimizes cost.\n",
    "- Limited to specific task types (e.g., SQL, Delta Live Tables).\n",
    "\n",
    "## Parameters\n",
    "\n",
    "### Job Parameters\n",
    "\n",
    "- Defined at the job level and available to all tasks.\n",
    "- Accessed using `dbutils.jobs.taskValues.get()` or widgets (for notebooks).\n",
    "- Example use: passing data source paths, environment configs.\n",
    "\n",
    "### Task Parameters\n",
    "\n",
    "- Defined and scoped per task.\n",
    "- Used when a task has input specific only to its logic.\n",
    "- Override job parameters if same key is used.\n",
    "\n",
    "## Scheduling\n",
    "\n",
    "- Jobs can be scheduled using cron expressions or simple intervals.\n",
    "- Options:\n",
    "  - Run once\n",
    "  - Run every N minutes/hours/days\n",
    "  - Advanced cron-style expressions\n",
    "\n",
    "## Triggers\n",
    "\n",
    "- Manual trigger (run now)\n",
    "- Scheduled trigger (based on time)\n",
    "- API-triggered (via REST API or other jobs)\n",
    "- Event-based (e.g., on data availability or external systems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac2770f7-556a-4e30-a6f2-62087bbf48a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Workflows.md",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
